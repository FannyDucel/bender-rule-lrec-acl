{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We import the py file which contains all the functions we created to preprocess the data before classifying them\n",
    "(especially to link the article to its category and studied language)\"\"\"\n",
    "from benderrule_tools import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_chemins_annotes_lrec = chemin_corpus(\"lrec_annotes2/*/*\",\"en\",\"lrec\")\n",
    "liste_lang = get_list_lang(\"en\")\n",
    "\n",
    "dico_br_app_lrec_a = br_appliquee(liste_chemins_annotes_lrec,liste_lang,\"english\")\n",
    "data_annote_all = dico_br_appliquee_phrases(liste_chemins_annotes_lrec,liste_lang, dico_br_app_lrec_a)\n",
    "\n",
    "with open(\"data_classif1_en420f.json\") as f:\n",
    "    data_annote = json.load(f)\n",
    "textes_en = [x[0] for x in data_annote]\n",
    "classes_en = [x[1] for x in data_annote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_report(textes_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annote = [[\"ACL\", \"420-acl_annotation_bender.json\"], [\"LREC\", \"550-lrec_annotation_bender.json\"]]\n",
    "\n",
    "with open(\"corpus_lremap.json\")as f:\n",
    "    names_LRE = json.load(f)\n",
    "    \n",
    "resultats_conf(data_annote, names_LRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg_saga_pen=l1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf ... 1.53 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    appliquée       0.92      0.96      0.94       704\n",
      "non-appliquée       0.47      0.32      0.38        84\n",
      "\n",
      "     accuracy                           0.89       788\n",
      "    macro avg       0.70      0.64      0.66       788\n",
      " weighted avg       0.87      0.89      0.88       788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count ... 2.68 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    appliquée       0.94      0.92      0.93       704\n",
      "non-appliquée       0.42      0.49      0.45        84\n",
      "\n",
      "     accuracy                           0.87       788\n",
      "    macro avg       0.68      0.70      0.69       788\n",
      " weighted avg       0.88      0.87      0.88       788\n",
      "\n",
      "LogReg_saga_pen=l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf ... 0.41 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    appliquée       0.94      0.02      0.04       704\n",
      "non-appliquée       0.11      0.99      0.19        84\n",
      "\n",
      "     accuracy                           0.13       788\n",
      "    macro avg       0.52      0.51      0.12       788\n",
      " weighted avg       0.85      0.13      0.06       788\n",
      "\n",
      "Count ... 0.42 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    appliquée       0.93      0.95      0.94       704\n",
      "non-appliquée       0.52      0.44      0.48        84\n",
      "\n",
      "     accuracy                           0.90       788\n",
      "    macro avg       0.73      0.70      0.71       788\n",
      " weighted avg       0.89      0.90      0.89       788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "classifiers = []\n",
    "for pen in [\"l1\", \"l2\"]:\n",
    "    classifiers.append([\"LogReg_saga_pen=%s\"%pen, \n",
    "                        LogisticRegression(multi_class=\"auto\", solver=\"saga\", \n",
    "                                           max_iter=500, class_weight=\"balanced\",\n",
    "                                          penalty = pen)])\n",
    "\n",
    "    \n",
    "vectorizers=[\n",
    "    [\"Tfidf\",TfidfVectorizer()],\n",
    "    [\"Count\", CountVectorizer()]\n",
    "]\n",
    "for nom_classif, algo in classifiers:\n",
    "  print(nom_classif)\n",
    "  for name, V in vectorizers:\n",
    "    X1_en = V.fit_transform(textes_en)\n",
    "    Train_V = V.fit(textes_en)\n",
    "    Y1_en = classes_en\n",
    "    X_train_en, X_test_en, y_train_en, y_test_en = train_test_split(X1_en, Y1_en, test_size=0.3, random_state=0)\n",
    "    start = time.perf_counter()\n",
    "    clf_en = algo.fit(X_train_en, y_train_en)\n",
    "    end = time.perf_counter()\n",
    "    print(name, \"... %.2f seconds\"%(round(end-start,3)))\n",
    "    pred_en = clf_en.predict(X_test_en)\n",
    "\n",
    "    conf_matrix_en = confusion_matrix(y_test_en, pred_en)\n",
    "    report_en = classification_report(y_test_en,pred_en)\n",
    "    print(report_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACL\n",
      "LREC\n"
     ]
    }
   ],
   "source": [
    "with open(\"corpus_lremap.json\")as f:\n",
    "    names_LRE = json.load(f)\n",
    "    \n",
    "stats_LRE = {}\n",
    "data_annote = [[\"ACL\", \"420-acl_annotation_bender.json\"], [\"LREC\", \"550-lrec_annotation_bender.json\"]]\n",
    "resultats = {}\n",
    "for conf_name, json_path in data_annote:\n",
    "    print(conf_name)\n",
    "    resultats[conf_name] = {\"Sentence\":[], \"Baseline\":[], \"Baseline_LRE\":[], \"Sentence_LRE\":[]}\n",
    "    with open(json_path) as f:\n",
    "        dic_conf = json.load(f)\n",
    "    for path_article, infos_article in dic_conf.items():\n",
    "        classe = infos_article[\"class\"]\n",
    "        if classe ==\"Non-déductible\" or classe ==\"Non-Aplicable\":\n",
    "            classe=\"N/A\"\n",
    "        chaine = read_file(path_article)\n",
    "        has_LRE = False\n",
    "        for name in names_LRE:\n",
    "            if name in chaine:\n",
    "                has_LRE = True\n",
    "                stats_LRE.setdefault(name,0)\n",
    "                stats_LRE[name]+=1\n",
    "        chaine = chaine.lower()\n",
    "        phrases = chaine.split(\".\")\n",
    "        a_tester = []\n",
    "        for phrase in phrases:\n",
    "            for mot in phrase.split():\n",
    "                for l in liste_lang:\n",
    "                    if mot == l:\n",
    "                        a_tester.append(phrase)\n",
    "        if len(a_tester)>0:\n",
    "            pred_baseline = \"Appliquée\"\n",
    "            pred_LRE = pred_baseline\n",
    "            if classe ==\"Déductible\":#pour avoir aussi les déductibles\n",
    "                pred_baseline = classe\n",
    "            X_test2 = Train_V.transform(a_tester)\n",
    "            Y_pred2 = clf_en.predict(X_test2)\n",
    "        else:\n",
    "            pred_baseline = \"N/A\"\n",
    "            Y_pred2= [[\"N/A\"]]\n",
    "            if has_LRE==True:\n",
    "                pred_LRE = \"Déductible\"\n",
    "            else:\n",
    "                pred_LRE = pred_baseline\n",
    "\n",
    "        NB_pos = len([x for x in Y_pred2 if x== \"appliquée\"])\n",
    "        if \"appliquée\" in Y_pred2:\n",
    "            pred_sentence = \"Appliquée\"\n",
    "            if classe ==\"Déductible\":#pour avoir aussi les déductibles\n",
    "                pred_sentence = classe\n",
    "        else:\n",
    "            pred_sentence = \"N/A\"\n",
    "        pred_sentence_LRE = pred_sentence\n",
    "        if pred_sentence!=\"Appliquée\":\n",
    "            if pred_LRE==\"Déductible\":\n",
    "                pred_sentence_LRE = pred_LRE\n",
    "            \"Sentence_LRE\"\n",
    "        resultats[conf_name][\"Sentence\"].append([classe, pred_sentence])\n",
    "        #, {\"Phr_lg\":len(a_tester), \"Phr_lg_pos\":NB_pos}])\n",
    "        resultats[conf_name][\"Baseline\"].append([classe, pred_baseline])\n",
    "        resultats[conf_name][\"Baseline_LRE\"].append([classe, pred_LRE])\n",
    "        resultats[conf_name][\"Sentence_LRE\"].append([classe, pred_sentence_LRE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Sentence\n",
      "\n",
      "LREC SpearmanrResult(correlation=0.66363633080393, pvalue=4.218059408516568e-71)\n",
      "ACL SpearmanrResult(correlation=0.7372822669269066, pvalue=5.007545573363368e-73)\n",
      "Applied\t\t & 0.908 & 0.986 & 0.946 & 440 & 0.855 & 0.974 & 0.911 & 230\\\\\n",
      "Deducible\t & 1.0 & 0.6 & 0.75 & 20 & 1.0 & 0.286 & 0.444 & 42\\\\\n",
      "N/A\t\t & 0.367 & 0.579 & 0.449 & 38 & 0.448 & 0.707 & 0.549 & 92\\\\\n",
      "macro avg\t & 0.569 & 0.541 & 0.536 & 550 & 0.576 & 0.492 & 0.476 & 419\\\\\n",
      "micro avg\t & 0.788 & 0.851 & 0.815 & 550 & 0.668 & 0.718 & 0.665 & 419\\\\\n",
      "--------------------\n",
      "Baseline\n",
      "\n",
      "LREC SpearmanrResult(correlation=0.625036153908324, pvalue=6.092385324772995e-61)\n",
      "ACL SpearmanrResult(correlation=0.507784425004818, pvalue=7.605421904649439e-29)\n",
      "Applied\t\t & 0.894 & 0.993 & 0.941 & 440 & 0.728 & 0.978 & 0.835 & 230\\\\\n",
      "Deducible\t & 1.0 & 0.65 & 0.788 & 20 & 1.0 & 0.595 & 0.746 & 42\\\\\n",
      "N/A\t\t & 0.396 & 0.5 & 0.442 & 38 & 0.435 & 0.402 & 0.418 & 92\\\\\n",
      "macro avg\t & 0.572 & 0.536 & 0.543 & 550 & 0.541 & 0.494 & 0.5 & 419\\\\\n",
      "micro avg\t & 0.779 & 0.853 & 0.812 & 550 & 0.596 & 0.685 & 0.625 & 419\\\\\n",
      "--------------------\n",
      "Baseline_LRE\n",
      "\n",
      "LREC SpearmanrResult(correlation=0.5625822751390871, pvalue=3.1552732269904694e-47)\n",
      "ACL SpearmanrResult(correlation=0.4918858320150052, pvalue=6.506698206563193e-27)\n",
      "Applied\t\t & 0.871 & 0.993 & 0.928 & 440 & 0.674 & 0.978 & 0.798 & 230\\\\\n",
      "Deducible\t & 0.2 & 0.25 & 0.222 & 20 & 0.298 & 0.405 & 0.343 & 42\\\\\n",
      "N/A\t\t & 0.435 & 0.263 & 0.328 & 38 & 0.536 & 0.163 & 0.25 & 92\\\\\n",
      "macro avg\t & 0.376 & 0.377 & 0.369 & 550 & 0.377 & 0.387 & 0.348 & 419\\\\\n",
      "micro avg\t & 0.734 & 0.822 & 0.773 & 550 & 0.517 & 0.613 & 0.527 & 419\\\\\n",
      "--------------------\n",
      "Sentence_LRE\n",
      "\n",
      "LREC SpearmanrResult(correlation=0.6622276448152492, pvalue=1.0532716945021771e-70)\n",
      "ACL SpearmanrResult(correlation=0.7337434056588434, pvalue=5.353605293365082e-72)\n",
      "Applied\t\t & 0.908 & 0.986 & 0.946 & 440 & 0.855 & 0.974 & 0.911 & 230\\\\\n",
      "Deducible\t & 0.459 & 0.85 & 0.596 & 20 & 0.42 & 0.69 & 0.523 & 42\\\\\n",
      "N/A\t\t & 0.371 & 0.342 & 0.356 & 38 & 0.489 & 0.467 & 0.478 & 92\\\\\n",
      "macro avg\t & 0.435 & 0.545 & 0.475 & 550 & 0.441 & 0.533 & 0.478 & 419\\\\\n",
      "micro avg\t & 0.769 & 0.844 & 0.803 & 550 & 0.619 & 0.706 & 0.657 & 419\\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "for class_name, dic_res_lrec in resultats[\"LREC\"].items():\n",
    "    print(\"-\"*20)\n",
    "    print(class_name+\"\\n\")\n",
    "    y_test, y_pred = [x[0] for x in dic_res_lrec], [x[1] for x in dic_res_lrec]\n",
    "    report_lrec = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(\"LREC\", str(spearmanr(y_test,y_pred)))\n",
    "\n",
    "    dic_res_acl = resultats[\"ACL\"][class_name]\n",
    "    y_test, y_pred = [x[0] for x in dic_res_acl], [x[1] for x in dic_res_acl]\n",
    "    report_acl = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(\"ACL\", str(spearmanr(y_test,y_pred)))\n",
    "\n",
    "    dic_classes = {\"Appliquée\":\"Applied\\t\", \"Déductible\":\"Deducible\", \"N/A\":\"N/A\\t\", \"macro avg\":\"macro avg\", \"weighted avg\":\"micro avg\"}\n",
    "    for classe in [\"Appliquée\", \"Déductible\", \"N/A\", \"macro avg\", \"weighted avg\"]:\n",
    "        liste_ligne = [dic_classes[classe]+\"\\t\"]\n",
    "        for this_res in [report_lrec, report_acl]:\n",
    "            for mesure in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "                liste_ligne.append(round(this_res[classe][mesure], 3))\n",
    "            liste_ligne.append(this_res[classe][\"support\"])\n",
    "        print(\" & \".join([str(x) for x in liste_ligne])+\"\\\\\\\\\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ESO': 21, 'UE': 92, 'RTE': 23, 'DUC': 70, 'Semi': 105, 'EAGLE': 20, 'NSP': 10, 'CID': 40, 'SUC': 10, 'AMI': 56, 'TUT': 8, 'AAC': 191, 'CSI': 25, 'Turkish': 48, 'GMA': 6, 'DMC': 3, 'DCR': 4, 'FEAT': 4, 'ACD': 6, 'DiSCo': 1, 'COW': 8, 'RSC': 4, 'BAT': 2, 'WSJ': 38, 'Wall Street Journal Corpus': 1, 'with parsing': 2, 'Unified Medical Language System': 8, 'Le Monde': 6, 'C 3': 24, 'Topic Detection and Tracking': 4, 'Switchboard': 17, 'NYT': 7, 'ELC': 3, 'PropBank': 20, 'British National Corpus': 140, 'BNC': 165, 'Argo': 1, 'UMLS': 24, 'ATB': 5, 'LDC2006T13': 2, 'MALLET': 3, 'PDT': 17, 'SUMO': 16, 'PMT': 3, 'SDG': 2, 'Automatic Content Extraction': 8, 'NUCLE': 4, 'RNC': 4, 'ukWaC': 5, 'TAC KBP': 6, 'MPQA': 14, 'MIM': 5, 'ODIN': 1, 'Audio visual': 1, 'Unofficial': 2, 'CSLU': 1, 'CHILDES': 3, 'CRPC': 2, 'PHOC': 1, 'MASC': 4, 'Le Robert': 1, 'Trésor de la Langue Française informatisé': 1, 'TLFi': 1, 'Suggested Upper Merged Ontology': 8, 'SAMA': 1, 'ESTER': 3, 'ERJ': 1, 'Russian National Corpus': 2, 'Serbian Wordnet': 1, 'DeWaC': 2, 'CLUTO': 1, 'EXMARaLDA': 3, 'Perl module': 2, 'CMDI': 7, 'SoNaR': 15, 'MLSA': 1, 'FCE': 3, 'NIST score': 1, 'Open Mind Indoor Common Sense': 1, 'WebCAGe': 1, 'Tübingen Treebank of Written German': 2, 'Postech Learner Corpus': 1, 'POLC': 1, 'SGJP': 1, 'Pattern Dictionary of English Verbs': 1, 'PDEV': 1, 'JWI': 1, 'CItA corpus': 1, 'Corpus Italiano di Apprendenti L1': 1, 'DeSR': 1, 'Corpus of Interactional Data': 4, 'BART': 3, 'PALinkA': 1, 'WDC': 1, 'ESLO': 1, 'MTO': 1, 'Lefff': 2, 'ISIP': 1, 'ACE 2005 Corpus': 1, 'GLPK': 1, 'WOLF': 1}\n"
     ]
    }
   ],
   "source": [
    "print(stats_LRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base+bal+saga Count 1-1\t 0.7080934840065763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base+bal+saga Count 3-7--char 0.6923557429530726\n",
      "base+bal+iter+saga Count 1-1\t 0.7049336036677809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rundimeco/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base+bal+iter+saga Count 3-7--char 0.6819094541163807\n",
      "LogisticRegr_Bal_C=0.05 Count 1-1\t 0.649761449640232\n",
      "LogisticRegr_Bal_C=0.05 Count 3-7--char 0.6883395162784177\n",
      "Decision Tree Count 1-1\t 0.6188954641539373\n",
      "Decision Tree Count 3-7--char 0.6295094957605853\n",
      "[0.6188954641539373, 'Decision Tree', 'Count 1-1\\t']\n",
      "[0.6295094957605853, 'Decision Tree', 'Count 3-7--char']\n",
      "[0.649761449640232, 'LogisticRegr_Bal_C=0.05', 'Count 1-1\\t']\n",
      "[0.6819094541163807, 'base+bal+iter+saga', 'Count 3-7--char']\n",
      "[0.6883395162784177, 'LogisticRegr_Bal_C=0.05', 'Count 3-7--char']\n",
      "[0.6923557429530726, 'base+bal+saga', 'Count 3-7--char']\n",
      "[0.7049336036677809, 'base+bal+iter+saga', 'Count 1-1\\t']\n",
      "[0.7080934840065763, 'base+bal+saga', 'Count 1-1\\t']\n"
     ]
    }
   ],
   "source": [
    "#### Tester vectoriseurs\n",
    "algo = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=500, class_weight=\"balanced\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Balanced est indispensable\n",
    "classifiers = [\n",
    "    #[\"base+bal\", LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", class_weight=\"balanced\")\n",
    "    #],\n",
    "    #[\"base+bal+iter\", LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=1000, class_weight=\"balanced\")\n",
    "    #],\n",
    "        [\"base+bal+saga\", LogisticRegression(multi_class=\"auto\", solver=\"saga\", class_weight=\"balanced\")\n",
    "    ],\n",
    "    [\"base+bal+iter+saga\", LogisticRegression(multi_class=\"auto\", solver=\"saga\", max_iter=1000, class_weight=\"balanced\")\n",
    "    ],\n",
    "    #[\"LogisticRegr_Bal_C=0.01\", LogisticRegression(multi_class=\"auto\", solver=\"saga\", C=0.01, n_jobs=2, max_iter=1000, class_weight=\"balanced\")],\n",
    "    [\"LogisticRegr_Bal_C=0.05\", LogisticRegression(multi_class=\"auto\", solver=\"saga\", C=0.05, n_jobs=2, max_iter=1000, class_weight=\"balanced\")],\n",
    "    [\"Decision Tree\", DecisionTreeClassifier()],\n",
    "    #[\"MNB\", MultinomialNB()],\n",
    "    #[\"RF_depth=10_estimators=100\", RandomForestClassifier(max_depth=10, n_estimators = 100, random_state=0)],\n",
    "]\n",
    "    \n",
    "\n",
    "#V = CountVectorizer(ngram_range=(3,5), analyzer=\"char\")\n",
    "list_vectorizer = [\n",
    "    [\"Count 1-1\\t\", CountVectorizer()],\n",
    "    [\"Count 3-7--char\", CountVectorizer(ngram_range=(3,7), analyzer=\"char\")],\n",
    "    #[\"tfidf\", TfidfVectorizer()],\n",
    "    #[\"tfidf 3-7--char\", TfidfVectorizer(ngram_range=(3,7), analyzer=\"char\")],\n",
    "]\n",
    "list_res = []\n",
    "for name, clf in classifiers:\n",
    "  for nom, this_V in list_vectorizer:\n",
    "    X1_en = this_V.fit_transform(textes_en)\n",
    "    Train_V = this_V.fit(textes_en)\n",
    "    Y1_en = classes_en\n",
    "    \n",
    "    X_train_en, X_test_en, y_train_en, y_test_en = train_test_split(X1_en, Y1_en, test_size=0.3, random_state=0)\n",
    "    start = time.perf_counter()\n",
    "    clf_new = clf.fit(X_train_en, y_train_en)\n",
    "    end = time.perf_counter()\n",
    "    #print(\"... %.2f seconds\"%(round(end-start,2)))\n",
    "    pred_en = clf_new.predict(X_test_en)\n",
    "\n",
    "    conf_matrix_en = confusion_matrix(y_test_en, pred_en)\n",
    "    #print(\"Classifieur 1\\n\",conf_matrix_en) #na/app 229/3 2/9\n",
    "    report_en = classification_report(y_test_en,pred_en,output_dict=True)\n",
    "    print(name, nom, report_en[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "    list_res.append([report_en[\"macro avg\"][\"f1-score\"], name, nom] )\n",
    "for a in sorted(list_res):\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
